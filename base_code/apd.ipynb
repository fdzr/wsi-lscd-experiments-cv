{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from common_generate_predictions import (\n",
    "    load_data,\n",
    "    calculate_correlation,\n",
    "    save_cv_results,\n",
    "    save_correlation\n",
    ")\n",
    "from custom_types import Results\n",
    "from cross_validation import cross_validation_for_dwug_es as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = [\"zs\", \"fs\", \"ct\"]\n",
    "PATH_TO_SAVE_RESULTS = \"../cv-apd-experiments-lscd\"\n",
    "path_data = \"../input\"\n",
    "LLMS = [\"llama3.1-8B\", \"mixtral-8xtb-v0.1\"]\n",
    "WIC_MODELS = [\"wic1\", \"wic2\", \"wic3\", \"wic4\", \"wic5\", \"wic6\", \"wic7\"]\n",
    "DATASETS = [\"dwug_es\", \"dwug_en\"]\n",
    "path_to_gold_data_en = \"../test_data_en.csv\"\n",
    "path_to_gold_data_es = \"../test_data_es.csv\"\n",
    "PATH_TO_TARGET_WORDS = {\n",
    "    \"dwug_es\": \"../test_data_es.csv\",\n",
    "    \"dwug_en\": \"../test_data_en.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(llm: str, dataset: str, prompts: list, wic_data: bool = False):\n",
    "    scores = {}\n",
    "\n",
    "    for p in prompts:\n",
    "        path_to_data = Path(f\"{path_data}/{llm}/{dataset}/{p}\")\n",
    "\n",
    "        if wic_data is False:\n",
    "            assert path_to_data.exists() is True, f\"{path_to_data} does not exist\"\n",
    "\n",
    "        scores[p] = load_data(path_to_data, wic_data)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(word: str, result: Results, parameters: dict, path_to_save: str):\n",
    "    df = pd.read_csv(path_to_save)\n",
    "\n",
    "    n_rows = df.shape[0]\n",
    "\n",
    "    df.loc[n_rows, \"word\"] = word\n",
    "    df.loc[n_rows, \"apd\"] = result.jsd\n",
    "    df.loc[n_rows, \"parameters\"] = str(parameters)\n",
    "\n",
    "    df.to_csv(path_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_APD(hyperparameters: dict, scores: pd.DataFrame, metadata: dict):\n",
    "    apd_per_word = {}\n",
    "\n",
    "    for word in scores[\"word\"].unique().tolist():\n",
    "        mask = scores[\"word\"] == word\n",
    "        score_per_word = scores[mask]\n",
    "\n",
    "        apd_per_word[word] = Results(\n",
    "            jsd=score_per_word[\"score\"].mean() * metadata[\"factor\"],\n",
    "            cluster_to_freq1=None,\n",
    "            cluster_to_freq2=None,\n",
    "        )\n",
    "\n",
    "        path_to_save_results = metadata[\"path_to_save_results\"]\n",
    "        kfold = metadata[\"kfold\"]\n",
    "        name_file = metadata[\"name_file\"]\n",
    "\n",
    "        save_results(\n",
    "            word,\n",
    "            apd_per_word[word],\n",
    "            hyperparameters,\n",
    "            f\"{path_to_save_results}/{kfold}_fold/{name_file}.csv\",\n",
    "        )\n",
    "\n",
    "    return apd_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    hyperparameters: list[dict],\n",
    "    scores: dict[str, pd.DataFrame],\n",
    "    train_set: list,\n",
    "    metadata: dict,\n",
    "):\n",
    "    max_spr = -10.0\n",
    "    best_configuration = None\n",
    "    dataset = metadata[\"dataset\"]\n",
    "    metadata[\"name_file\"] = \"results_training_set\"\n",
    "\n",
    "    for hyperparameter in hyperparameters:\n",
    "        score_filtered = scores[hyperparameter[\"prompt\"]]\n",
    "        mask = score_filtered[\"word\"].isin(train_set)\n",
    "        train_scores = score_filtered[mask]\n",
    "\n",
    "        apd = get_APD(hyperparameter, train_scores, metadata)\n",
    "        spr = calculate_correlation(apd, PATH_TO_TARGET_WORDS[dataset])\n",
    "\n",
    "        save_correlation(\n",
    "            spr,\n",
    "            hyperparameter,\n",
    "            f\"{metadata['path_to_save_results']}/{metadata['kfold']}_fold/training.csv\",\n",
    "        )\n",
    "\n",
    "        if spr > max_spr:\n",
    "            max_spr = spr\n",
    "            best_configuration = hyperparameter\n",
    "\n",
    "    return {\"optimal_parameters\": best_configuration, \"max_spr_lscd\": max_spr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(\n",
    "    hyperparameters: dict,\n",
    "    scores: dict[str, pd.DataFrame],\n",
    "    test_set: list,\n",
    "    metadata: dict,\n",
    "):\n",
    "    dataset = metadata[\"dataset\"]\n",
    "    metadata[\"name_file\"] = \"results_testing_set\"\n",
    "\n",
    "    for hyperparameter in [hyperparameters]:\n",
    "        score_filtered = scores[hyperparameter[\"prompt\"]]\n",
    "        mask = score_filtered[\"word\"].isin(test_set)\n",
    "        test_scores = score_filtered[mask]\n",
    "\n",
    "        apd_per_words = get_APD(hyperparameter, test_scores, metadata)\n",
    "        spr = calculate_correlation(apd_per_words, PATH_TO_TARGET_WORDS[dataset])\n",
    "\n",
    "        save_correlation(\n",
    "            spr,\n",
    "            hyperparameter,\n",
    "            f\"{metadata['path_to_save_results']}/{metadata['kfold']}_fold/testing.csv\",\n",
    "        )\n",
    "        return spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_using_apd(\n",
    "    d: str, scores: dict, hyperparameters: list[dict], metadata: dict\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    for index in cv[d].keys():\n",
    "        train_set = cv[d][index][\"train\"]\n",
    "        test_set = cv[d][index][\"test\"]\n",
    "\n",
    "        metadata[\"kfold\"] = index\n",
    "        metadata[\"dataset\"] = d\n",
    "\n",
    "        configuration = train(hyperparameters, scores, train_set, metadata)\n",
    "        test_corr = eval(configuration[\"optimal_parameters\"], scores, test_set, metadata)\n",
    "\n",
    "        results[index] = {\"training\": configuration, \"testing\": test_corr}\n",
    "        path_to_save = (\n",
    "            f\"{metadata['path_to_save_results']}/{index}_fold/verbose_results.txt\"\n",
    "        )\n",
    "        with open(path_to_save, \"a\") as f_out:\n",
    "            f_out.write(\"best parameters for training: \\n\")\n",
    "            f_out.write(f\"  {configuration['optimal_parameters']}\\n\")\n",
    "            f_out.write(f\"training [spr_lscd]: \\n\")\n",
    "            f_out.write(f\"  {configuration['max_spr_lscd']}\\n\")\n",
    "\n",
    "            f_out.write(\"\\n\")\n",
    "\n",
    "            f_out.write(f\"testing [spr_lscd]:\\n\")\n",
    "            f_out.write(f\"  {test_corr}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiments(llm: str, hyperparameters: dict, metadata: dict):\n",
    "    for d in DATASETS:\n",
    "        scores = load_scores(\n",
    "            llm if metadata[\"wic_data\"] is False else \"wic-scores\",\n",
    "            d,\n",
    "            metadata[\"prompts\"],\n",
    "            wic_data=metadata[\"wic_data\"],\n",
    "        )\n",
    "        metadata[\"path_to_save_results\"] = f\"{PATH_TO_SAVE_RESULTS}/{llm}/{d}\"\n",
    "\n",
    "        results = cross_validation_using_apd(d, scores, hyperparameters, metadata)\n",
    "        save_cv_results(results, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [{\"prompt\": p} for p in PROMPTS]\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"factor\": -1.0,\n",
    "    \"wic_data\": False,\n",
    "    \"prompts\": PROMPTS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_experiments(\"llama3.1-8B\", hyperparameters, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"corr-apd.txt\", \"w\") as f_out:\n",
    "\n",
    "#     for llm in LLMS:\n",
    "#         f_out.write(f\"{llm}:\\n\")\n",
    "\n",
    "#         for d in DATASETS:\n",
    "#             f_out.write(f\"  {d}:\\n\")\n",
    "\n",
    "#             for p in PROMPTS:\n",
    "#                 f_out.write(f\"    {p}\\n\")\n",
    "#                 print(f\"    {p}\")\n",
    "#                 p = Path(f\"{path_data}/{llm}/{d}/{p}\")\n",
    "\n",
    "#                 assert p.exists() is True, f\"{p} does not exist\"\n",
    "\n",
    "#                 data = load_data(p)\n",
    "#                 words_apd = {}\n",
    "\n",
    "#                 for word in data[\"word\"].unique().tolist():\n",
    "#                     mask = data[\"word\"] == word\n",
    "#                     score_or_distance = data[mask]\n",
    "\n",
    "#                     words_apd[word] = Results(\n",
    "#                         jsd=score_or_distance.select_dtypes(include=\"number\").mean(),\n",
    "#                         cluster_to_freq1=None,\n",
    "#                         cluster_to_freq2=None,\n",
    "#                     )\n",
    "\n",
    "#                 corr = calculate_correlation(words_apd, PATH_TO_TARGET_WORDS[d])\n",
    "#                 f_out.write(f\"      corr: {corr}\")\n",
    "#                 f_out.write(f\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WiC models APD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"corr-apd-wic.txt\", \"w\") as f_out:\n",
    "\n",
    "    for d in DATASETS:\n",
    "        f_out.write(f\"{d}:\\n\")\n",
    "\n",
    "        for wic in WIC_MODELS:\n",
    "            f_out.write(f\"  {wic}:\\n\")\n",
    "\n",
    "            p = Path(f\"{path_data}/wic-scores/{d}\")\n",
    "            assert p.exists() is True, f\"{p} does not exist\"\n",
    "\n",
    "            path_to_wic_model = p / f\"{wic}\"\n",
    "\n",
    "            data = load_data(path_to_wic_model, wic_data=True)\n",
    "            words_apd = {}\n",
    "\n",
    "            for word in data[\"word\"].unique().tolist():\n",
    "                mask = data[\"word\"] == word\n",
    "                score_or_distance = data[mask]\n",
    "\n",
    "                words_apd[word] = Results(\n",
    "                    jsd=score_or_distance[\"score\"].mean() * -1.0,\n",
    "                    cluster_to_freq1=None,\n",
    "                    cluster_to_freq2=None,\n",
    "                )\n",
    "\n",
    "            corr = calculate_correlation(words_apd, PATH_TO_TARGET_WORDS[d])\n",
    "            f_out.write(f\"    corr: {corr}\")\n",
    "            f_out.write(f\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
